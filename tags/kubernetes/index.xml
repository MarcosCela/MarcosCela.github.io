<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kubernetes on Wiki</title><link>https://wiki.mcela.dev/tags/kubernetes/</link><description>Recent content in kubernetes on Wiki</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 12 Feb 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://wiki.mcela.dev/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>Setting up Kubernetes ApiServer auditing with ELK stack</title><link>https://wiki.mcela.dev/guides/kubernetes/elk-audit/</link><pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate><guid>https://wiki.mcela.dev/guides/kubernetes/elk-audit/</guid><description>What is Kubernetes auditing? Kubernetes auditing provides a set of records or log entries documenting what actions have affected the system. Basically, we have access to every API call, along with some metadata.
This can be really useful:
It provides you with security-relevant data, you can see WHO changed WHAT and WHEN. It can be used to detect bad configurations or possible attacks if your API server is exposed to the internet (if your apiserver shows a large ammount of 401 Unauthorized responses, someone might be trying to access your cluster with invalid credentials).</description></item><item><title>Kubernetes DNS problems in heterogeneous clusters</title><link>https://wiki.mcela.dev/guides/kubernetes/kubernetes-dns-problems-in-heterogeneous-clusters/</link><pubDate>Mon, 04 Feb 2019 00:00:00 +0000</pubDate><guid>https://wiki.mcela.dev/guides/kubernetes/kubernetes-dns-problems-in-heterogeneous-clusters/</guid><description>The symptoms Pods will not correctly perform DNS resolution, seemingly at random. One day everything is fine, the next day everything is on fire. Kube-dns pods show some restarts (2-3) in the last few days, these restarts are correlated to the problems, and indicate a bad restart. This means that SOMETIMES, when the kube-dns pod restarted, it did so in a bad state or configuration. Logs on kube-dns failed containers show an upstream DNS resolver of 127.</description></item></channel></rss>